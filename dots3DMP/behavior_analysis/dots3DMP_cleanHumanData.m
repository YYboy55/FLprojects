% take a human data struct generated by PLDAPS_preprocessing and clean it up a
% bit, for further analysis

% for SfN2021 poster, using:
% human_20190626-20191231: non-RT data, n=5 (_nonRT_clean)
% human_20200203-20211020: RT data, n=7 (RT_clean)

% as of Jan 2022, human_20200203-20220113

% processing steps
% 1. remove sessions/subjects (e.g. training sessions) to consolidate data
% 2. remove breakfixes, training trials etc
% 3. remove v short blocks e.g. n<30
% 4. index coh - different cohs per subj/session, standardize as 1/2 (lo/hi)
% 5. clean up headings/modalities
% 6. normalize conf ratings! (and RT?)
% 7. save cleaned data!

clear all; close all

conftask = 1; % 1=colorbars, 2=PDW
normalize = 1;

% specify which (previously saved) mat file to load
 
subject = 'human';
paradigm = 'dots3DMP';
RTtask = 1;

if ~RTtask,  dateRange = 20190625:20191231; % non-RT
% else,        dateRange = 20200213:20220317; % RT 2
% else,        dateRange = 20200213:20211020; % RT 1
else,          dateRange = 20200213:20210526; % RT 0

end


folder = '/Users/stevenjerjian/Desktop/FetschLab/PLDAPS_data/dataStructs/';
file = [subject '_' num2str(dateRange(1)) '-' num2str(dateRange(end)) '.mat'];
load([folder file], 'data');

fields2remove = {'reward','confRT','insertTrial','PDW','oneTargChoice','oneTargTrial','oneConfTargTrial'};
for f = 1:length(fields2remove)
    try data = rmfield(data,fields2remove{f}); end
end
 

%%
% some new useful vars
for k = 1:length(data.filename)
    data.date(k,1) = str2double(data.filename{k}(9:16));
    data.subjDate{k,:} = [data.subj{k} data.filename{k}(9:16)];
end

%% 1. manual excludes (e.g., training days)

if ~RTtask
    excludes_filename = {};
    excludes_subjDate = {'AAW20190604','AAW20190612','LLV20190717','LLV20190723','LLV20190730'...
        'IWT20190719','IWT20190726','CXD20190724','EMF20190724','EMF20190906','EMF20190909'};  %
    excludes_subj = {'XRJ'}; % remaining subjs should be AAW, CXD, EMF, IWT, LLV
    subjs = {'AAW','LLV','IWT','CXD','EMF'}; % non-RT
else
    excludes_filename = {'humanVZC20200229dots3DMP1239','humanKVU20210928dots3DMP1702','humanDCJ20211006dots3DMP1444','humanDCJ20211006dots3DMP1450','humanDJB20211202dots3DMP1609'}; % KVU 2021/09/28 first block, DCJ 2021/10/06 first block, DJB 2021/12/02 first block
    excludes_subjDate = {'FRK20200216','FRK20200223','VZC20200222','DRF20210824','DRF20210826',...
        'ABF20210819','ABF20210826','DCJ20211001','KVU20210921','KVU20210924','SBG20210929','SBG20211001','SBG20211006'};
    excludes_subj = {'NEX', 'NKT', 'TST'};
%     subjs = {'DRH','SJJ','LLV','IPQ','FRK','VZC','DRF','ABF','DCJ','KVU','SBG'}; % RT
%     subjs = {'DRH','SJJ','LLV','DRF','ABF','DCJ','KVU','SBG'}; % RT
    subjs = {'DRH','SJJ','LLV','IPQ','FRK','DRF','KVU','SBG','DJB'}; % RT

end
% subjs = {'AAW' 'LLV' 'CXD' 'DRH' 'IPQ' 'SJJ' 'VZC'}; % all 'good' data (pre and post RT)

% default/testing, including everything
% excludes_filename = {};
% excludes_subjDate = {};
% excludes_subj = {};
% subjs = unique(data.subj);

removethese = ismember(data.filename,excludes_filename) | ismember(data.subjDate,excludes_subjDate) | ismember(data.subj,excludes_subj) | ~ismember(data.subj,subjs);
fnames = fieldnames(data);
for F = 1:length(fnames)
    data.(fnames{F})(removethese) = [];
end

% now this should reflect only good data, per spreadsheet:
blocks = unique(data.filename);

% remove invalid trials (fixation breaks (which gives nans), and obvious testing trials, signaled by very large confidence (saccade
% endpoint) values
removethese = isnan(data.choice) | isnan(data.conf) | data.conf>3 ;
fnames = fieldnames(data);
for F = 1:length(fnames)
    data.(fnames{F})(removethese) = [];
end

% quick look at blocks, for when some need to be excluded
[blocks,nTrialsByBlock] = blockCounts(data.filename);

% we can be pretty sure blocks with <N trials (say, 30) are to be discarded
N = 30;
removethese = ismember(data.filename,blocks(nTrialsByBlock<N));
for F = 1:length(fnames)
    data.(fnames{F})(removethese) = [];
end

% quick look at blocks again, shouldn't be any with <N trials
[blocks,nTrialsByBlock,blockInds] = blockCounts(data.filename);

%% organize cohs variable

% subjects should be tested with two cohs, low and high wrt vestibular
% may be different for different subjects, and actual values are arbitrary, 
% so let's just index them as 1 (low) and 2 (high), within each block.

% some pre-RT data has 0 coh (?), and 1 or 3 cohs within a 'good' data
% block

%{
% save original 'raw' coherences
data.cohRaw = data.coherence;
data.coherence = nan(size(data.coherence));

ucohs_byBlock = nan(numel(blocks),3);
for b=1:length(blocks)
    temp = data;
    blockTrials = strcmp(data.filename,blocks{b});
    temp.cohRaw(~blockTrials) = [];
    [ucohs,~,inds] = unique(temp.cohRaw,'sorted');
    
    ucohs_byBlock(b,1:length(ucohs)) = ucohs';
    
    if length(ucohs)==3 % only used once (.3,.5,.9 ... pool the two low ones)
        temp.cohRaw(temp.cohRaw<0.6) = 0.4;
        [ucohs,~,inds] = unique(temp.cohRaw,'sorted');
    elseif length(ucohs)==1 % ignore? only once, 0 was used, so should default to cohI = 1
%         continue
    end
    
    data.coherence(blockTrials) = inds;

end
%}

data.coherence(data.coherence<0.5) = 0.4;
data.coherence(data.coherence>=0.5)= 0.7;

%{
% SJ 03-2023 this is only for a certain file
% blocks 10 and 13 only have two low cohs ~0.05 and 0.15, block 11 has only
% 0 coh?? discard these
fnames = fieldnames(data);
removethese = ismember(blockInds,[10 11 13]);
for F = 1:length(fnames)
    data.(fnames{F})(removethese) = [];
end
%}

%% organize headings variable
% reassign headings to indices -N...0...+N, or fixed headings, within subject block (i.e.
% arbitrary) ?

uhdgs_byBlock = nan(numel(blocks),12);
for b=1:length(blocks)
    temp = data;
    blockTrials = strcmp(data.filename,blocks{b});
    temp.heading(~blockTrials) = [];
    [uhdgs,~,inds] = unique(temp.heading,'sorted');
    uhdgs_byBlock(b,1:length(uhdgs)) = uhdgs'; 
end

% simple for now, just group by hand...

data.heading(abs(data.heading)<0.01) = 0;

% fix data.correct (see function description for issue!)
data.correct = dots3DMPCorrectTrials(data.choice,data.heading,data.delta);
    
% hdgVals = [1 2 3];
hdgVals = logspace(log10(1.25),log10(10),3); % arbitrary

data.heading(data.heading<-0.5 & data.heading>=-2) = -hdgVals(1);
data.heading(data.heading>0.5 & data.heading<=2) = hdgVals(1);

data.heading(data.heading<-2 & data.heading>-6) = -hdgVals(2);
data.heading(data.heading>2 & data.heading<6) = hdgVals(2);

data.heading(data.heading<-6 & data.heading>=-12) = -hdgVals(3);
data.heading(data.heading>6 & data.heading<=12) = hdgVals(3);

% sanity check
% uhdgs_byBlock2 = nan(numel(blocks),12);
% for b=1:length(blocks)
%     temp = data;
%     blockTrials = strcmp(data.filename,blocks{b});
%     temp.heading(~blockTrials) = [];
%     [uhdgs,~,inds] = unique(temp.heading,'sorted');
%     uhdgs_byBlock2(b,1:length(uhdgs)) = uhdgs'; 
% end

%% cull data

% this block is mostly obsolete given the standardization we've done on
% cohs and headings above, but leave it in here anyway for now

mods = unique(data.modality); 
cohs = unique(data.coherence); 

% remove the rest
removethese = ~ismember(data.coherence,cohs) & data.modality~=1;
for F = 1:length(fnames)
    data.(fnames{F})(removethese) = [];
end
    
% the coh assigned to vestib trials (as a placeholder) depends on which
% cohs were shown in a particular block, so we need to standardize it:
% actually this is obsolete here since we already standardized cohs above
% data.coherence(data.modality==1) = cohs(1);

deltas = unique(data.delta); % aka conflict angle
hdgs = unique(data.heading);
% hdgs(hdgs==0) = []; % remove 0 heading trials if not enough? nah, do it
% later if we want

% remove the rest
removethese = ~ismember(data.heading,hdgs);
for F = 1:length(fnames)
    data.(fnames{F})(removethese) = [];
end

% final look at blocks 
[blocks,nTrialsByBlock] = blockCounts(data.filename);

%% final pass, remove some 'bad' RTs, and subjects with too few trials

if ~RTtask
    subjs = unique(data.subj);
    subjs2keep = 1:5;
%     subjs2keep = [1 3 4 5];
else
    RTlims = [0.25 2.5];
    fnames = fieldnames(data);
    removethese = data.RT < RTlims(1) | data.RT > RTlims(2);
    for f=1:length(fnames), data.(fnames{f})(removethese) = []; end
    
    % should standardize RTs too for better analysis when pooling subject data?
    % let's check the distributions first
    
    subjs = unique(data.subj);
    subjs2keep = [1 3 5 8 9];
%     subjs2keep = [1 3 8 9];

end
% fnames = fieldnames(data);
% removethese = ~ismember(data.subj,subjs(subjs2keep));
% for f=1:length(fnames), data.(fnames{f})(removethese) = []; end

%% normalize confidence ratings, *within subject*

if normalize

data_orig = data;
usubj = unique(data.subj);
for s = 1:length(usubj)
    data = data_orig;
    removethese = ~strcmp(data.subj,usubj{s});
    for F = 1:length(fnames)
        data.(fnames{F})(removethese) = [];
    end    
    
    % subtract min and divide by max
    
    % Across all choices
    data.conf = (data.conf - min(data.conf)) / max((data.conf - min(data.conf)));
    
    % SJ 07-2021
    % SEPARATELY FOR LEFT AND RIGHT CHOICES 
    
    % ah, but what if subjects are genuinely more confident on one side
    % that the other (this would be forcibly removed from the dataset...)
    % so don't do this
    
%     Lconf = data.conf(data.choice==1);
%     Rconf = data.conf(data.choice==2);
% 
%     data.conf(data.choice==1) = (Lconf - min(Lconf)) / max((Lconf - min(Lconf)));
%     data.conf(data.choice==2) = (Rconf - min(Rconf)) / max((Rconf - min(Rconf)));
    

    % OR
    
    % subtract/divide by *means*
%     minOfMeans = nanmin(nanmin(nanmin(nanmin(confMean))));
%     data.conf = data.conf - minOfMeans;
%     dots3DMP_parseData
%     maxOfMeans = nanmax(nanmax(nanmax(nanmax(confMean))));
%     data.conf = data.conf / maxOfMeans;

    % OR
    
    % simply cap at 1/0
    % data.conf(data.conf>1) = 1;
    % data.conf(data.conf<0) = 0;
    
    % Normalize RTs too
%     data.RT = (data.RT - min(data.RT)) / max((data.RT - min(data.RT)));

    % append each subj to a new data struct
    if s==1
        data_new = data;
    else
        for F = 1:length(fnames)
            eval(['data_new.' fnames{F} '(end+1:end+length(data.date)) = data.' fnames{F} ';']);
        end
    end
end
data = data_new;
data.confRaw = data_orig.conf;
% data.RTorig  = data_orig.RT;
clear data_new data_orig
end




% remove zero heading trials, insufficient data
% for f=1:length(fnames), data.(fnames{f})(data.heading==0) = []; end



%%
if ~RTtask
    save([folder file(1:end-4) '_nonRT_clean.mat'],'data')
else
    save([folder file(1:end-4) '_RT_clean2.mat'],'data')
end
% save([file(1:end-4) '_clean.mat'],'data')
fprintf('done.\n')


%%%%%%%%%%% % helper function
function [blocks,nTrialsByBlock,blockInds] = blockCounts(filenames)
[blocks,~,blockInds] = unique(filenames);
nTrialsByBlock = nan(length(blocks),1);
for u = 1:length(blocks)
    nTrialsByBlock(u) = sum(ismember(filenames,blocks(u)));
end
end

